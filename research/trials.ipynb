{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ajana/Code/MLProjects/Heart-Disease-Detection'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart.components.data_ingesion import DataIngestion\n",
    "from heart.entity.artifact_entity import DataIngestionArtifact\n",
    "from heart.entity.config_entity import TrainingPipelineConfig,DataIngestionConfig\n",
    "from heart.constant.training_pipeline import TARGET_COLUMN,SCHEMA_FILE_PATH\n",
    "from heart.utils import read_yaml_file\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = TrainingPipelineConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = DataIngestionConfig(training_pipeline_config=training_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion = DataIngestion(data_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ingestion.read_data_from_feature_store()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dframe = pd.read_csv(data_config.feature_store_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'gender', 'impluse', 'pressurehight', 'pressurelow', 'glucose',\n",
       "       'kcm', 'troponin', 'class'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dframe.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/home/ajana/Code/MLProjects/Heart-Disease-Detection/artifact/12_25_2023_19_03_57/data_ingestion/feature_store/Heart Attack.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Index' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Index' object is not callable"
     ]
    }
   ],
   "source": [
    "df.columns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "_schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat = _schema_config[\"categorical_column\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from heart.exception import HeartException\n",
    "from heart.logger import logging\n",
    "from heart.constant.training_pipeline import TARGET_COLUMN,SCHEMA_FILE_PATH\n",
    "from heart.entity.artifact_entity import (DataValidationArtifact,\n",
    "                                          DataTrasformationArtifact)\n",
    "from heart.entity.config_entity import DataTransformationConfig\n",
    "\n",
    "from heart.ml.model.estimator import TargetValueMapping\n",
    "from heart.utils import save_numpy_array_data, save_object\n",
    "from heart.utils import read_yaml_file\n",
    "\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self,\n",
    "                 data_transformation_config: DataTransformationConfig,\n",
    "                 data_validation_artifact: DataValidationArtifact):\n",
    "        try:\n",
    "            self.data_transformation_config = data_transformation_config\n",
    "            self.data_validation_artifact = data_validation_artifact\n",
    "            self._schema_config = read_yaml_file(file_path=SCHEMA_FILE_PATH)\n",
    "        except Exception as e:\n",
    "            raise HeartException(e,sys)\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(file_path: str) -> pd.DataFrame:\n",
    "        try:\n",
    "            return pd.read_csv(file_path)\n",
    "        except Exception as e:\n",
    "            raise HeartException(e,sys)\n",
    "    \n",
    "    def get_data_transformer_object(self) -> ColumnTransformer:\n",
    "        try:\n",
    "            numerical_cols = self._schema_config[\"numerical_columns\"],\n",
    "            categorical_cols = self._schema_config[\"categorical_columns\"]\n",
    "            numerical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', RobustScaler())\n",
    "            ])\n",
    "\n",
    "            categorical_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "            ])\n",
    "            logging.info(f\"Categorical columns: {categorical_cols}\")\n",
    "            logging.info(f\"Numerical columns: {numerical_cols}\")\n",
    "\n",
    "            preprocessor = ColumnTransformer(\n",
    "                transformers=[\n",
    "                    ('num', numerical_transformer, numerical_cols),\n",
    "                    ('cat', categorical_transformer, categorical_cols)\n",
    "                ])\n",
    "\n",
    "            return preprocessor\n",
    "        except Exception as e:\n",
    "            raise HeartException(e,sys)\n",
    "\n",
    "    def initiate_data_transformation(self) -> DataTrasformationArtifact:\n",
    "        try:\n",
    "            # logging.info(\"Initiating data transformation\")\n",
    "            print (self._schema_config[\"numerical_columns\"])\n",
    "            # preprocessor_obj = self.get_data_transformer_object()\n",
    "            # print (preprocessor_obj)\n",
    "            # logging.info(\"Data transformation object created\")\n",
    "            train_df = DataTransformation.read_data(self.data_validation_artifact.valid_train_file_path)  \n",
    "            test_df = DataTransformation.read_data(self.data_validation_artifact.valid_test_file_path)\n",
    "            print (self.data_validation_artifact.valid_train_file_path)\n",
    "            input_feature_train_df = train_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            target_feature_train_df = train_df[TARGET_COLUMN]\n",
    "            target_feature_train_df = target_feature_train_df.repalce(TargetValueMapping()).to_dict()\n",
    "            logging.info(\"Got train features and test features of Training dataset\")\n",
    "            input_feature_test_df = test_df.drop(columns=[TARGET_COLUMN],axis=1)\n",
    "            target_feature_test_df = test_df[TARGET_COLUMN]\n",
    "            target_feature_test_df = target_feature_test_df.repalce(TargetValueMapping()).to_dict()\n",
    "            logging.info(\"Got train features and test features of Testing dataset\")\n",
    "\n",
    "            logging.info(\n",
    "                \"Applying preprocessing object on training dataframe and testing dataframe\"\n",
    "            )\n",
    "\n",
    "            input_feature_train_arr = preprocessor_obj.fit_transform(input_feature_train_df)\n",
    "            logging.info(\n",
    "                \"Used the preprocessor object to fit transform the train features\"\n",
    "            )\n",
    "            input_feature_test_arr = preprocessor_obj.transform(input_feature_test_df)\n",
    "            logging.info(\"Used the preprocessor object to transform the test features\")\n",
    "\n",
    "            logging.info(\"Applying SMOTETomek on Training dataset\")\n",
    "            smt = SMOTETomek(sampling_strategy=\"minority\",random_state=42)\n",
    "            input_feature_train_arr, target_feature_train_df = smt.fit_resample(input_feature_train_arr, target_feature_train_df)   \n",
    "            logging.info(\"Applied SMOTETomek on testing dataset\")\n",
    "\n",
    "            logging.info(\"Created train array and test array\")\n",
    "\n",
    "            train_arr = np.c_[input_feature_train_arr, np.array(target_feature_train_df)]\n",
    "            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\n",
    "            logging.info(\"Data transformation completed\")\n",
    "\n",
    "            save_object(\n",
    "                file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                obj=preprocessor_obj\n",
    "            )\n",
    "\n",
    "            logging.info(\"Saving train array and test array\")\n",
    "            save_numpy_array_data(\n",
    "                file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                array=train_arr\n",
    "            )\n",
    "            save_numpy_array_data(\n",
    "                file_path=self.data_transformation_config.transformed_test_file_path,\n",
    "                array=test_arr\n",
    "            )\n",
    "\n",
    "            logging.info(\"Saved train array and test array\")\n",
    "            logging.info(\"Creating DataTransformationArtifact\")\n",
    "            data_transformation_artifact =  DataTrasformationArtifact(\n",
    "                transformed_object_file_path=self.data_transformation_config.transformed_object_file_path,\n",
    "                transformed_train_file_path=self.data_transformation_config.transformed_train_file_path,\n",
    "                transformed_test_file_path=self.data_transformation_config.transformed_test_file_path,\n",
    "            )\n",
    "            return data_transformation_artifact\n",
    "        except Exception as e:\n",
    "            HeartException(e,sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heart.exception import HeartException\n",
    "from heart.logger import logging\n",
    "from heart.components.data_ingesion import DataIngestion\n",
    "from heart.components.data_validation import DataValidation\n",
    "from heart.components.data_transformation import DataTransformation\n",
    "from heart.entity.artifact_entity import DataIngestionArtifact,DataValidationArtifact,DataTrasformationArtifact\n",
    "from heart.entity.config_entity import TrainingPipelineConfig,DataIngestionConfig,DataValidationConfig,DataTransformationConfig\n",
    "import sys\n",
    "\n",
    "\n",
    "class TrainingPipeline:\n",
    "\n",
    "    def __init__(self,training_config: TrainingPipelineConfig):\n",
    "        self.training_config = training_config\n",
    "        self.data_ingestion_config = DataIngestionConfig(training_pipeline_config=self.training_config)\n",
    "        self.data_validation_config = DataValidationConfig(training_pipeline_config=self.training_config)\n",
    "        self.data_transformation_config = DataTransformationConfig(training_pipeline_config=self.training_config)\n",
    "\n",
    "\n",
    "    def start_data_ingestion(self) -> DataIngestionArtifact:\n",
    "        try:\n",
    "            data_ingestion_config = self.data_ingestion_config\n",
    "            data_ingestion = DataIngestion(data_ingestion_config=data_ingestion_config)\n",
    "            data_ingestion_artifact = data_ingestion.initiate_data_ingestion()\n",
    "            logging.info(\n",
    "                \"Exited the start_data_ingestion method of TrainPipeline class\"\n",
    "            )\n",
    "            return data_ingestion_artifact\n",
    "\n",
    "        except Exception as e:\n",
    "            raise HeartException(e, sys)\n",
    "\n",
    "    def start_data_validation(self, data_ingestion_artifact: DataIngestionArtifact) -> DataValidationArtifact:\n",
    "        try:\n",
    "            data_validation_config = self.data_validation_config\n",
    "            data_validation = DataValidation(data_ingestion_artifact=data_ingestion_artifact,\n",
    "                                             data_validation_config=data_validation_config)\n",
    "            data_validation_artifact = data_validation.initiate_data_validation()\n",
    "            logging.info(\n",
    "                \"Exited the start_data_validation method of TrainPipeline class\"\n",
    "            )\n",
    "            return data_validation_artifact\n",
    "        except Exception as e:\n",
    "            raise HeartException(e, sys)\n",
    "        \n",
    "    def start_data_transformation(self, data_validation_artifact: DataValidationArtifact) -> DataTrasformationArtifact:\n",
    "        try:\n",
    "            data_transformation_config = self.data_transformation_config\n",
    "            data_tranformation = DataTransformation(data_transformation_config=data_transformation_config,\n",
    "                                                    data_validation_artifact=data_validation_artifact) \n",
    "            data_tranformation_artifact = data_tranformation.initiate_data_transformation()\n",
    "            return data_tranformation_artifact\n",
    "        except Exception as e:\n",
    "            HeartException(e, sys)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        try:\n",
    "            data_ingestion_artifact = self.start_data_ingestion()\n",
    "            data_validation_artifact = self.start_data_validation(data_ingestion_artifact=data_ingestion_artifact)\n",
    "            data_transformation_artifact = self.start_data_transformation(data_validation_artifact=data_validation_artifact)\n",
    "            # model_trainer_artifact = self.start_model_trainer(data_transformation_artifact)\n",
    "            # model_eval_artifact = self.start_model_evaluation(data_validation_artifact, model_trainer_artifact)\n",
    "        except Exception as e:\n",
    "            raise HeartException(e, sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 .kaggle/kaggle.json'\n"
     ]
    }
   ],
   "source": [
    "from heart.pipeline.training_pipeline import TrainingPipeline\n",
    "from heart.entity.config_entity import TrainingPipelineConfig\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    training_config= TrainingPipelineConfig()\n",
    "    training_pipeline = TrainingPipeline(training_config=training_config)\n",
    "    training_pipeline.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
